{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import and preprocess data\n",
    "train_set = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header = None)\n",
    "test_set = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test',\n",
    "                      skiprows = 1, header = None) # Make sure to skip a row for the test set\n",
    "\n",
    "col_labels = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', \n",
    "              'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "             'wage_class']\n",
    "train_set.columns = col_labels\n",
    "test_set.columns = col_labels\n",
    "\n",
    "train_nomissing = train_set.replace(' ?', np.nan).dropna()\n",
    "test_nomissing = test_set.replace(' ?', np.nan).dropna()\n",
    "train_nomissing['wage_class'] = train_nomissing.wage_class.replace({' <=50K.': ' <=50K', ' >50K.':' >50K'})\n",
    "test_nomissing['wage_class'] = test_nomissing.wage_class.replace({' <=50K.': ' <=50K', ' >50K.':' >50K'})\n",
    "\n",
    "combined_set = pd.concat([train_nomissing, test_nomissing], axis = 0) # Stacks them vertically\n",
    "for feature in combined_set.columns: # Loop through all columns in the dataframe\n",
    "    if combined_set[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
    "        combined_set[feature] = pd.Categorical(combined_set[feature]).codes # Replace strings with an integer\n",
    "\n",
    "final_train = combined_set[:train_nomissing.shape[0]] # Up to the last initial training set row\n",
    "final_test = combined_set[train_nomissing.shape[0]:] # Past the last initial training set row\n",
    "\n",
    "\n",
    "y_train = final_train.pop('wage_class')\n",
    "y_test = final_test.pop('wage_class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Up parameter tuning ranges.\n",
    "cv_params = {'max_depth': [3,5,7], #3-10\n",
    "             'min_child_weight': [1,3,5]}\n",
    "            #colsample_bytree #0.5-1.0\n",
    "            #lambda = 1\n",
    "            #alpha #regularization term (useful for high dimensionality)\n",
    "            # scale_pos_weight [default=1] counters class imbalance\n",
    "#Set up initial parameters\n",
    "ind_params = {'learning_rate': 0.1, #0.01 - 0.2\n",
    "              'n_estimators': 1000, \n",
    "              'seed':0, \n",
    "              'subsample': 0.8, #0.5-1.0\n",
    "              'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'} #'mutli:softmax' (will reqwuire setting num_class= ) ,multi:softprob\n",
    "                #eval_metric rmse – root mean square error\n",
    "                    # mae – mean absolute error\n",
    "                    # logloss – negative log-likelihood\n",
    "                    # error – Binary classification error rate (0.5 threshold)\n",
    "                    # merror – Multiclass classification error rate\n",
    "                    # mlogloss – Multiclass logloss\n",
    "                    # auc: Area under the curve\n",
    "# Create\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', \n",
    "                             cv = 5, \n",
    "                             n_jobs = -1) \n",
    "# Optimize for accuracy since that is the metric used in the Adult Data Set notation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum number of trees for this learning rate. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.\n",
    "2. Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.\n",
    "3. Tune regularization parameters (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.\n",
    "4. Lower the learning rate and decide the optimal parameters ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "optimized_GBM.fit(final_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GridSearchCV(cv=5, error_score='raise',\n",
    "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "       min_child_weight=1, missing=None, n_estimators=1000, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8),\n",
    "       fit_params={}, iid=True, n_jobs=-1,\n",
    "       param_grid={'min_child_weight': [1, 3, 5], 'max_depth': [3, 5, 7]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimized_GBM.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
