{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e124bbc296d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#LOAD DATA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtests\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "from sklearn.feature_extraction\n",
    "from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sklearn.preprocessing' from '/home/gilad/VMachine/lib/python2.7/site-packages/sklearn/preprocessing/__init__.pyc'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprossesing`\n",
    "from sklearn import preprocessing\n",
    "#Binarize data (set feature values to 0 or 1) according to a threshold\n",
    "preprocessing.Binarizer(self, threshold=0.0, copy=True) \n",
    "\n",
    "#Constructs a transformer from an arbitrary callable.\n",
    "preprocessing.FunctionTransformer(func=None, validate=True, accept_sparse=False, pass_y=False)\n",
    "\n",
    "#Imputation transformer for completing missing values.\n",
    "preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0, verbose=0, copy=True)\n",
    "\n",
    "\n",
    "preprocessing.KernelCenterer??\n",
    "\n",
    "#Binarize labels in a one-vs-all fashion\n",
    "preprocessing.LabelBinarizer(self, neg_label=0, pos_label=1, sparse_output=False)\n",
    "\n",
    "#Encode labels with value between 0 and n_classes-1.\n",
    "preprocessing.LabelEncoder()       \n",
    "\n",
    "#Scale each feature by its maximum absolute value.\n",
    "preprocessing.MaxAbsScaler(copy=True)     \n",
    "\n",
    "#Transforms features by scaling each feature to a given range.\n",
    "preprocessing.MinMaxScaler(self, feature_range=(0, 1), copy=True)\n",
    "\n",
    "#Transform between iterable of iterables and a multilabel format\n",
    "preprocessing.MultiLabelBinarizer(classes=None, sparse_output=False) \n",
    "\n",
    "#Normalize samples individually to unit norm.)\n",
    "preprocessing.Normalizer(norm='l2', copy=True)\n",
    "\n",
    "#Encode categorical integer features using a one-hot aka one-of-K scheme.)\n",
    "preprocessing.OneHotEncoder(n_values='auto', categorical_features='all', dtype=<type 'float'>, sparse=True, handle_unknown='error')\n",
    "\n",
    "#Generate polynomial and interaction features.)\n",
    "preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "\n",
    "#Scale features using statistics that are robust to outliers.)\n",
    "preprocessing.RobustScaler(with_centering=True, with_scaling=True, copy=True)\n",
    "\n",
    "#Standardize features by removing the mean and scaling to unit variance)\n",
    "preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)  #MAY NEED FURTH WHITENING\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train/Valid/Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes at least 2 arguments (1 given)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9c32bad3d674>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProjectedGradientNMF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomizedPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mdecomposition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseCoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m decomposition.SparsePCA(self, n_components=None, alpha=1, ridge_alpha=0.01, max_iter=1000, tol=1e-08, \n\u001b[0;32m     30\u001b[0m                         method='lars', n_jobs=1, U_init=None, V_init=None, verbose=False, random_state=None)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes at least 2 arguments (1 given)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#Feature_Selection\n",
    "from sklearn import decomposition\n",
    "#Finds a dictionary (a set of atoms) that can best be used to represent datausing a sparse code.\n",
    "decomposition.DictionaryLearning(n_components=None, alpha=1, max_iter=1000, tol=1e-08, fit_algorithm='lars', \n",
    "                                 transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, \n",
    "                                 n_jobs=1, code_init=None, dict_init=None, verbose=False, split_sign=False, random_state=None)\n",
    "# Factor Analysis (FA)\n",
    "\n",
    "# A simple linear generative model with Gaussian latent variables.\n",
    "\n",
    "# The observations are assumed to be caused by a linear transformation of\n",
    "# lower dimensional latent factors and added Gaussian noise.\n",
    "# Without loss of generality the factors are distributed according to a\n",
    "# Gaussian with zero mean and unit covariance. The noise is also zero mean\n",
    "# and has an arbitrary diagonal covariance matrix.\n",
    "\n",
    "# If we would restrict the model further, by assuming that the Gaussian\n",
    "# noise is even isotropic (all diagonal entries are the same) we would obtain\n",
    "# :class:`PPCA`.\n",
    "\n",
    "# FactorAnalysis performs a maximum likelihood estimate of the so-called\n",
    "# `loading` matrix, the transformation of the latent variables to the\n",
    "# observed ones, using expectation-maximization (EM).\n",
    "decomposition.FactorAnalysis (n_components=None, tol=0.01, copy=True, max_iter=1000, \n",
    "                             noise_variance_init=None, svd_method='randomized', iterated_power=3, random_state=0)\n",
    "    \n",
    "#FastICA: a fast algorithm for Independent Component Analysis.\n",
    "decomposition.FastICA(n_components=None, algorithm='parallel', whiten=True, fun='logcosh', \n",
    "                      fun_args=None, max_iter=200, tol=0.0001, w_init=None, random_state=None)\n",
    "\n",
    "#Incremental principal components analysis (IPCA).\n",
    "decomposition.IncrementalPCA(n_components=None, whiten=False, copy=True, batch_size=None)\n",
    "\n",
    "#Kernel Principal component analysis (KPCA) Non-linear dimensionality reduction through the use of kernels (see:ref:`metrics`).\n",
    "decomposition.KernelPCA(n_components=None, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None, alpha=1.0, \n",
    "                        fit_inverse_transform=False, eigen_solver='auto', tol=0, max_iter=None, remove_zero_eig=False)\n",
    "\n",
    "# Latent Dirichlet Allocation with online variational Bayes algorithm\n",
    "decomposition.LatentDirichletAllocation(n_topics=10, doc_topic_prior=None, topic_word_prior=None, learning_method='online', learning_decay=0.7, \n",
    "                                        learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, \n",
    "                                        mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=1, verbose=0, random_state=None)\n",
    "\n",
    "#Mini-batch dictionary learning\n",
    "decomposition.MiniBatchDictionaryLearning(n_components=None, alpha=1, n_iter=1000, fit_algorithm='lars', n_jobs=1, batch_size=3, shuffle=True,\n",
    "                                          dict_init=None, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, \n",
    "                                          verbose=False, split_sign=False, random_state=None)\n",
    "\n",
    "#Mini-batch Sparse Principal Components Analysis\n",
    "# Finds the set of sparse components that can optimally reconstruct\n",
    "# the data.  The amount of sparseness is controllable by the coefficient`\n",
    "# of the L1 penalty, given by the parameter alpha.\n",
    "decomposition.MiniBatchSparsePCA(n_components=None, alpha=1, ridge_alpha=0.01, n_iter=100, callback=None, batch_size=3, verbose=False, \n",
    "                                 shuffle=True, n_jobs=1, method='lars', random_state=None)\n",
    "\n",
    "\n",
    "# Non-Negative Matrix Factorization (NMF)\n",
    "# Find two non-negative matrices (W, H) whose product approximates the non-\n",
    "# negative matrix X. This factorization can be used for example for\n",
    "# dimensionality reduction, source separation or topic extraction.\n",
    "decomposition.NMF(n_components=None, init=None, solver='cd', tol=0.0001, max_iter=200, random_state=None, alpha=0.0, l1_ratio=0.0, \n",
    "                  verbose=0, shuffle=False, nls_max_iter=2000, sparseness=None, beta=1, eta=0.1)\n",
    "\n",
    "# Principal component analysis (PCA):\n",
    "# Linear dimensionality reduction using Singular Value Decomposition of the\n",
    "# data and keeping only the most significant singular vectors to project the\n",
    "# data to a lower dimensional space.\n",
    "# It only works for dense arrays and is not scalable to large dimensional data.\n",
    "decomposition.PCA(n_components=None, copy=True, whiten=False)\n",
    "\n",
    "\n",
    "# Non-Negative Matrix Factorization (NMF):\n",
    "# Find two non-negative matrices (W, H) whose product approximates the non-\n",
    "# negative matrix X. This factorization can be used for example for\n",
    "# dimensionality reduction, source separation or topic extraction.\n",
    "decomposition.ProjectedGradientNMF()\n",
    "\n",
    "#Principal component analysis (PCA) using randomized SVD\n",
    "decomposition.RandomizedPCA(n_components=None, copy=True, iterated_power=3, whiten=False, random_state=None)\n",
    "\n",
    "# Finds a sparse representation of data against a fixed, precomputeddictionary.\n",
    "decomposition.SparseCoder(dictionary, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, split_sign=False, n_jobs=1)\n",
    "\n",
    "#Finds the set of sparse components that can optimally reconstruct\n",
    "# the data.  The amount of sparseness is controllable by the coefficient\n",
    "# of the L1 penalty, given by the parameter alpha.\n",
    "decomposition.SparsePCA(self, n_components=None, alpha=1, ridge_alpha=0.01, max_iter=1000, tol=1e-08, \n",
    "                        method='lars', n_jobs=1, U_init=None, V_init=None, verbose=False, random_state=None)\n",
    "\n",
    "# Dimensionality reduction using truncated SVD (aka LSA).\n",
    "decomposition.TruncatedSVD(n_components=2, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilad/VMachine/lib/python2.7/site-packages/sklearn/utils/__init__.py:75: DeprecationWarning: Class ProjectedGradientNMF is deprecated; It will be removed in release 0.19. Use NMF instead.'pg' solver is still available until release 0.19.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a395c4d99cb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Finds a sparse representation of data against a fixed, precomputeddictionary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mdecomposition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseCoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform_algorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'omp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform_n_nonzero_coefs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_sign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#Finds the set of sparse components that can optimally reconstruct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Whitening\n",
    "decomposition.PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RUN Many Models and score them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
